import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier



df = pd.read_csv("nasa_processed.csv")
df.head()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, roc_auc_score, classification_report
)

# Show all columns for debugging
pd.set_option('display.max_columns', None)

# Check if your DataFrame is already in memory or load it if needed
# If it's already in memory from another notebook, skip this
# df = pd.read_csv("your_cleaned_dataset.csv")  # Optional

df.head()



sensor_cols = [
    'sensor_measurement_2',
    'sensor_measurement_3',
    'sensor_measurement_4',
    'sensor_measurement_7'
]



from sklearn.preprocessing import StandardScaler

# Drop rows with any missing values in selected sensors
df_multi = df.dropna(subset=sensor_cols).copy()

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_multi[sensor_cols])



from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.01, random_state=42)
df_multi['multi_anomaly'] = iso_forest.fit_predict(X_scaled)

# Convert -1 (anomaly), 1 (normal) to binary: 1 = anomaly
df_multi['multi_anomaly'] = df_multi['multi_anomaly'].map({1: 0, -1: 1})



import matplotlib.pyplot as plt

df_unit = df_multi[df_multi['engine_id'] == 1]

plt.figure(figsize=(12, 6))
plt.plot(df_unit['cycle'], df_unit['sensor_measurement_2'], label='Sensor 2', alpha=0.5)
plt.scatter(
    df_unit[df_unit['multi_anomaly'] == 1]['cycle'],
    df_unit[df_unit['multi_anomaly'] == 1]['sensor_measurement_2'],
    color='red', marker='x', label='Multisensor Anomaly'
)
plt.title('Multisensor Isolation Forest Anomalies (Engine 1)')
plt.xlabel('Cycle')
plt.ylabel('Sensor Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



features = [
    'sensor_measurement_2', 'sensor_measurement_2_ma', 'sensor_measurement_2_diff',
    'sensor_measurement_3', 'sensor_measurement_4', 'sensor_measurement_7'
]

df_model = df.dropna(subset=features + ['anomaly']).copy()
X = df_model[features]
y = df_model['anomaly']

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X, y)



import seaborn as sns

importances = clf.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()



from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay

y_pred = clf.predict(X)
y_prob = clf.predict_proba(X)[:, 1]  # probability for class 1

# Confusion Matrix
cm = confusion_matrix(y, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title('Confusion Matrix - Random Forest')
plt.show()



fpr, tpr, thresholds = roc_curve(y, y_prob)
roc_auc = roc_auc_score(y, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Anomaly Detection')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



print(df.columns)



X = df.drop("anomaly", axis=1)
y = df["anomaly"]


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)



from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [None, 10, 20, 30, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2'],  # Removed 'auto'
    'bootstrap': [True, False]
}

# Initialize model
rf = RandomForestClassifier(random_state=42)

# Randomized Search
rf_random = RandomizedSearchCV(estimator=rf,
                               param_distributions=param_grid,
                               n_iter=50,
                               cv=5,
                               verbose=2,
                               random_state=42,
                               n_jobs=-1)

# Fit to training data
rf_random.fit(X_train, y_train)

# Store the best model
best_rf = rf_random.best_estimator_



print("Best Parameters:", rf_random.best_params_)
print("Best Score (CV accuracy):", rf_random.best_score_)



from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_pred = best_rf.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))



import matplotlib.pyplot as plt
import numpy as np

# Get feature importances from best_rf
importances = best_rf.feature_importances_
features = X.columns

# Sort by importance
indices = np.argsort(importances)[::-1]
top_n = 15  # Show top 15 features

# Plot
plt.figure(figsize=(10, 6))
plt.title("Top 15 Feature Importances (Random Forest)", fontsize=14)
plt.bar(range(top_n), importances[indices][:top_n], align="center", color="royalblue")
plt.xticks(range(top_n), [features[i] for i in indices[:top_n]], rotation=45, ha="right", fontsize=10)
plt.ylabel("Importance Score", fontsize=12)
plt.tight_layout()
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()



    !pip install xgboost



import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV



xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5]
}



use_label_encoder=False,


xgb_clf = XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)



xgb_clf = XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)

xgb_random = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=xgb_param_grid,
    n_iter=30,
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

xgb_random.fit(X_train, y_train)

best_xgb = xgb_random.best_estimator_



print("Best XGBoost Parameters:", xgb_random.best_params_)
print("Best CV Accuracy (XGBoost):", xgb_random.best_score_)
                                                                    


from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_pred_xgb = best_xgb.predict(X_test)

print("Test Accuracy (XGBoost):", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report (XGBoost):\n", classification_report(y_test, y_pred_xgb))
print("\nConfusion Matrix (XGBoost):\n", confusion_matrix(y_test, y_pred_xgb))



import joblib

# Save Random Forest model
joblib.dump(best_rf, 'random_forest_model.pkl')

# Save XGBoost model
joblib.dump(best_xgb, 'xgboost_model.pkl')



# Load and predict using saved Random Forest model
loaded_rf = joblib.load('random_forest_model.pkl')
y_loaded_pred = loaded_rf.predict(X_test)

# Confirm it's working
from sklearn.metrics import accuracy_score
print("Loaded RF Accuracy:", accuracy_score(y_test, y_loaded_pred))



import matplotlib.pyplot as plt
import numpy as np

# Get importances from XGBoost
xgb_importances = best_xgb.feature_importances_
features = X.columns
indices = np.argsort(xgb_importances)[::-1]
top_n = 15  # Show top 15 features

# Plot
plt.figure(figsize=(10, 6))
plt.title("Top 15 Feature Importances (XGBoost)", fontsize=14)
plt.bar(range(top_n), xgb_importances[indices][:top_n], color='darkorange', align="center")
plt.xticks(range(top_n), [features[i] for i in indices[:top_n]], rotation=45, ha="right", fontsize=10)
plt.ylabel("Importance Score", fontsize=12)
plt.tight_layout()
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()



# Save your test data (X_test) to a CSV file
X_test.to_csv("sample_input.csv", index=False)




