import pandas as pd

df = pd.read_csv('../data/train_FD001_cleaned.csv')
df.head()



from sklearn.preprocessing import MinMaxScaler

feature_cols = [col for col in df.columns if 'sensor' in col]

scaler = MinMaxScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])



from sklearn.model_selection import train_test_split

X = df[feature_cols]
y = df['RUL']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)



from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")



from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)



y_rf_pred = rf_model.predict(X_test)

rf_mae = mean_absolute_error(y_test, y_rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, y_rf_pred))

print(f"Random Forest MAE: {rf_mae:.2f}")
print(f"Random Forest RMSE: {rf_rmse:.2f}")



pip install xgboost



from xgboost import XGBRegressor

xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

y_xgb_pred = xgb_model.predict(X_test)

xgb_mae = mean_absolute_error(y_test, y_xgb_pred)
xgb_rmse = np.sqrt(mean_squared_error(y_test, y_xgb_pred))

print(f"XGBoost MAE: {xgb_mae:.2f}")
print(f"XGBoost RMSE: {xgb_rmse:.2f}")



import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_rf_pred[:100], label='Predicted RUL (RF)', marker='x')
plt.title('RUL Prediction: Actual vs Random Forest')
plt.xlabel('Sample Index')
plt.ylabel('Remaining Useful Life')
plt.legend()
plt.grid(True)
plt.show()




